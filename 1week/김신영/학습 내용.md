## 제목 : 1주차 - 1. PyTorch 설치 및 PyTorch의 Tensor 데이터 타입

### 강의 키워드
- 텐서 생성, 텐서 연산, PyTorch 설치

### 강의 내용
PyTorch의 기본적인 텐서 생성과 연산 방법을 학습했습니다. 다양한 텐서 조작 방법을 연습했습니다.  
특히, 텐서 연산 중 행렬곱 연산이 기억에 남습니다. 행렬곱 연산을 하는 방법엔 3가지가 있습니다:  
1. `torch.mm(tensor, tensor)`  
2. `torch.matmul(tensor, tensor)`  
3. `tensor @ tensor`


---
## 1주차 - 2.LinearRegression + Model save & load

### 강의 키워드
- 선형회귀, LinearRegressionModel, torch.nn.Module, 데이터 로딩

### 강의 내용
이번 강의에서는 PyTorch를 활용한 선형회귀 모델을 구현하고 훈련하는 방법을 학습했습니다. `torch.nn.Module`을 상속하여 `LinearRegressionModel` 클래스를 만들고, 데이터를 로딩해 모델을 훈련시켰습니다. 학습이 완료된 모델을 저장하고, 이후에 불러와서 사용하는 방법까지 실습했습니다.  

모델을 학습시킬 때는 반복적으로 비슷한 과정을 수행합니다.  
1. **데이터를 모델에 입력**: 이때 `model.forward` 메서드를 호출하거나 단순히 `model`이라고 작성해도 무방합니다.  
2. **손실 함수 계산**: 훈련 중 손실 함수를 사용해 오차를 계산합니다.  
3. **기울기 초기화**: `optimizer.zero_grad()`를 호출하여 이전의 기울기를 초기화하고 데이터의 정제를 돕습니다.  
4. **역전파**: `train_loss.backward()`를 통해 역전파를 실행하여 모델의 파라미터를 학습합니다.  
5. **파라미터 업데이트**: `optimizer.step()`을 호출하여 모델의 파라미터를 업데이트합니다.  

이 과정을 에폭 100으로 설정하면, 100번 반복하여 모델을 학습시킵니다.

---

## 1주차 - 3. Binary Classification + nn.Sequential

### 강의 키워드
- 이진 분류, nn.Sequential, 시그모이드 함수, ReLU 함수, 활성화 함수의 필요성

### 강의 내용
이번 강의에서는 이진 분류 문제를 해결하기 위한 신경망 모델을 PyTorch로 구현했습니다. `nn.Sequential`을 사용하여 모델을 구성하고, 활성화 함수로 시그모이드 함수와 ReLU 함수를 적용하는 방법을 배웠습니다. 활성화 함수의 필요성에 대해 이해하고, 이를 통해 비선형성을 모델에 도입하여 성능을 향상시킬 수 있음을 확인했습니다.  
특히, `torch.nn.BCEWithLogitsLoss()`와 `torch.nn.BCELoss()`가 기억에 남습니다. `BCEWithLogitsLoss`는 시그모이드 함수를 내부에서 적용한 후 BCE 손실을 계산하는 반면, `BCELoss`는 시그모이드가 이미 적용된 값을 입력으로 받기 때문에 미리 시그모이드 함수를 적용하지 않으면 오류가 날 수 있었습니다.

